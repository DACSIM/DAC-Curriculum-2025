{"cells":[{"cell_type":"markdown","metadata":{"id":"eT6jTWqA_eeM"},"source":["# Sentiment Analysis 2"]},{"cell_type":"markdown","metadata":{"id":"dl2ErmA4DcoN"},"source":["## Table of Contents:\n","Sentiment Analysis of Movie Reviews:\n","1. EDA\n","2. Data Preprocessing\n","3. Feature Extraction\n","4. ML modelling"]},{"cell_type":"markdown","metadata":{"id":"1fChb-R-Fu9I"},"source":["### Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ioLRjA94FiyG"},"outputs":[],"source":["# Linear algebra\n","import numpy as np\n","# EDA\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","# NLTK libraries\n","import nltk\n","nltk.download('all')    # After running all, comment out this line to stop redownloading nltk every time\n","# Stopwords\n","from nltk.corpus import stopwords\n","# Stemmer & Lemmatizer\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import LancasterStemmer,WordNetLemmatizer\n","from nltk.stem import WordNetLemmatizer\n","# Wordcloud\n","from wordcloud import WordCloud,STOPWORDS\n","# Tokenizer\n","from nltk.tokenize import word_tokenize,sent_tokenize\n","from nltk.tokenize.toktok import ToktokTokenizer\n","# RE\n","import re,string,unicodedata\n","# Bag of Words\n","from textblob import TextBlob\n","from textblob import Word\n","# Feature Extraction\n","from sklearn.model_selection import train_test_split, cross_val_score,StratifiedShuffleSplit\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelBinarizer\n","# ML models\n","from sklearn.linear_model import LogisticRegression,SGDClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","# Metrics\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n","from sklearn.metrics import ConfusionMatrixDisplay\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","# Web Scraping tool\n","from bs4 import BeautifulSoup"]},{"cell_type":"markdown","metadata":{"id":"Lz7-hE3dQu4x"},"source":["### Importing dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PeScqdmrPKpV"},"outputs":[],"source":["# data = pd.read_csv('IMDB.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3QB8JQn_eeb"},"outputs":[],"source":["# # Extend the dataframe display size\n","# pd.options.display.max_colwidth = 110"]},{"cell_type":"markdown","metadata":{"id":"lPZ-wdnUS4C8"},"source":["### Inspecting dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lrwVLJCQ4Bc"},"outputs":[],"source":["# data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xceQYBrzSLKS"},"outputs":[],"source":["# data.shape"]},{"cell_type":"markdown","metadata":{"id":"rs7l-jq4TKmE"},"source":["## 1. EDA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zRUYusYlTIYc"},"outputs":[],"source":["# # Summary of the dataset\n","# data.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HdEh23MkTQ_Z"},"outputs":[],"source":["# # Counting sentiments\n","# data['sentiment'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"En8UoW5RUz1x"},"source":["## 2. Data Preprocessing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzAPFuhF7ibM"},"outputs":[],"source":["# # Quick visualisation of dataset (First 5 + Last 5 rows)\n","# data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AxFmxMZL_eei"},"outputs":[],"source":["# # Make a copy of the 'data' dataframe to work off from\n","# data_IMDB = data.copy()"]},{"cell_type":"markdown","metadata":{"id":"WMYoGsJq_eei"},"source":["### Tokenizing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XY1V4mz_Tf1m"},"outputs":[],"source":["# Initialize the tokenizer\n","\n","\n","# Setting English stopwords\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGxBnkil6u4N"},"outputs":[],"source":["# Example text to tokenize\n","text = \"This is an example sentence for tokenization.\"\n","\n","# Tokenize the text\n","\n","\n","# # Print the tokens\n","# print(tokens)"]},{"cell_type":"markdown","metadata":{"id":"0YcKKbTZXkdX"},"source":["### Text Processor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZaeg2-nV7aJ"},"outputs":[],"source":["# Removing the html strips\n","def strip_html(text):\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    return soup.get_text()\n","\n","# Removing the square brackets\n","def remove_between_square_brackets(text):\n","    return re.sub('\\[[^]]*\\]', '', text)\n","\n","# Removing special characters\n","def remove_special_characters(text, remove_digits=True):\n","    pattern = r'[^a-zA-z0-9\\s]'\n","    text = re.sub(pattern,'',text)\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sptmkrsl7v9A"},"outputs":[],"source":["# Example of our 3 defined functions\n","html_text = \"<p>This is <b>bold</b> and <i>italic</i>.</p>\"\n","\n","\n","\n","input_text = \"This is [some text] with [multiple] sets of [square brackets].\"\n","\n","\n","\n","input_text = \"Hello, @world! 123\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NobuMs_1YHKo"},"outputs":[],"source":["# Removing the noisy text\n","def denoise_text(text):\n","    text = strip_html(text)\n","    text = remove_between_square_brackets(text)\n","    text = remove_special_characters(text)\n","    return text\n","\n","# Apply function on review column\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fCsrPNxe_eek"},"outputs":[],"source":["# data_IMDB['review']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"st6343ngY31g"},"outputs":[],"source":["# Lemmatizing the text\n","def simple_lemmatize(text):\n","    lemmatizer = WordNetLemmatizer()\n","    text = ' '.join({lemmatizer.lemmatize(word) for word in text.split()})\n","    return text\n","\n","# Apply function on review column\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1m6gf1RC1H8"},"outputs":[],"source":["# Example of lemmatizing text using sample sentence\n","input_text = \"I am running and eating. The cars are running fast.\"\n","\n"]},{"cell_type":"markdown","metadata":{"id":"M2J-v8LfHwZp"},"source":["### Initial Data-Preprocessing Verdict\n","Lemmatizer is not as accurate as we want it to be\n","- Sentences are garbled and in a mess"]},{"cell_type":"markdown","metadata":{"id":"spiAHLce_eel"},"source":["### Part-Of-Speech (POS) tagging\n","Implement Part-Of-Speech (POS) tagging to improve accuracy.\n","\n","This helps the algorithm understand the grammatical structure and meaning of a text.\n","\n","For example, consider the sentence: \"The cat is sleeping on the mat.\"\n","\n","POS tagging would assign the following tags:\n","- \"The\" - determiner (DT)\n","- \"cat\" - noun (NN)\n","- \"is\" - verb (VBZ)\n","- \"sleeping\" - verb (VBG)\n","- \"on\" - preposition (IN)\n","- \"the\" - determiner (DT)\n","- \"mat\" - noun (NN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64tl9OmVGP7y"},"outputs":[],"source":["# from nltk import pos_tag\n","\n","# # Download NLTK resources\n","# nltk.download('punkt')\n","# nltk.download('wordnet')\n","# nltk.download('averaged_perceptron_tagger')\n","\n","# # Function to process the pos_tag\n","# def get_wordnet_pos(tag):\n","#     if tag.startswith('N'):\n","#         return 'n'  # Noun\n","#     elif tag.startswith('V'):\n","#         return 'v'  # Verb\n","#     elif tag.startswith('R'):\n","#         return 'r'  # Adverb\n","#     elif tag.startswith('J'):\n","#         return 'a'  # Adjective\n","#     else:\n","#         return 'n'  # Default to noun for unknown or uncategorized words\n","\n","# # Redefining the lemmatizer function\n","# def simple_lemmatize(text):\n","#     lemmatizer = WordNetLemmatizer()\n","#     tokens = word_tokenize(text)\n","#     pos_tags = pos_tag(tokens)\n","#     lemmatized_tokens = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in pos_tags]\n","#     lemmatized_text = ' '.join(lemmatized_tokens)\n","#     return lemmatized_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jiiP5IcNHDkv"},"outputs":[],"source":["# Test if the new function works as intended\n","input_text = \"I am running and eating. The cars are running fast.\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_35d4QVOOhG"},"outputs":[],"source":["# # Can we apply the new function on review column?\n","# data_IMDB['review'] = data_IMDB['review'].apply(simple_lemmatize)\n","# # We are not going to do it this way; computationally expensive, time consuming"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eadnMU3TZS73"},"outputs":[],"source":["# # Set stopwords to English\n","# stop = set(stopwords.words('english'))\n","# print(stop)\n","\n","# # Removing the stopwords\n","# def remove_stopwords(text, is_lower_case=False):\n","#     tokens = tokenizer.tokenize(text)\n","#     tokens = [token.strip() for token in tokens]\n","#     if is_lower_case:\n","#         filtered_tokens = [token for token in tokens if token not in stopword]\n","#     else:\n","#         filtered_tokens = [token for token in tokens if token.lower() not in stopword]\n","#     filtered_text = ' '.join(filtered_tokens)\n","#     return filtered_text\n","\n","# # Example to test out our stopword-removing function\n","# input_text = \"This is an example sentence with some stopwords.\"\n","# filtered_text = remove_stopwords(input_text, is_lower_case=True)\n","# print(filtered_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dhovTtzODXE"},"outputs":[],"source":["# Apply function on 'review' column\n"]},{"cell_type":"markdown","metadata":{"id":"5cli3CZ2cfnk"},"source":["### Text Normalisation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pU4Dl2a8cjI7"},"outputs":[],"source":["# Set a variable for the normalized dataframe and add the data_IMDB\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72xBxKCGi267"},"outputs":[],"source":["# norm_data_IMDB.shape"]},{"cell_type":"markdown","metadata":{"id":"MomAYMLdleo-"},"source":["## 3. Feature Extraction"]},{"cell_type":"markdown","metadata":{"id":"XLy9vhfxg4kf"},"source":["### Method 1: Bag of Words\n","\n","The \"Bag of Words\" (BoW) model is a common and simple representation used in natural language processing (NLP) and information retrieval.\n","\n","It's a way of converting text data into numerical vectors that can be used by machine learning algorithms\n","\n","TLDR: Based on the raw word counts and is suitable when you want to capture the frequency of words in a document."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmNYt8w9P5io"},"outputs":[],"source":["# Example documents in list form\n","documents = [\"This is the first document.\",\n","              \"This document is the second document.\",\n","              \"And this is the third one.\",\n","              \"Is this the first document?\"]\n","\n","# Create an instance of the CountVectorizer class, where ngram ranges from 1 word to 3 words\n","# Unigram = singular word / Bigram = 2 words\n","\n","\n","# Fit and transform the documents into a Bag of Words representation\n","\n","\n","# Get the feature names (words) that correspond to the columns in the Bag of Words matrix\n","\n","\n","# Convert the Bag of Words matrix to an array for better visualization\n","\n","\n","# DataFrame for better visualization\n","\n","\n","# Display the DataFrame\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43qMqyzyvPN6"},"outputs":[],"source":["# # Fitting our data into the CountVectorizer\n","# vect = CountVectorizer(ngram_range=(1,3)).fit(norm_data_IMDB['review'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eY7GWKlPwZDT"},"outputs":[],"source":["# Getting the feature names from the vectorised features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92o5NyXJUfl8"},"outputs":[],"source":["# feature_names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdbMi9f7acuY"},"outputs":[],"source":["# norm_data_IMDB['review'].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ixbcLk4wtIh"},"outputs":[],"source":["# Extract the feature 'review'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ilqr94ZyRgKQ"},"outputs":[],"source":["# X_cv.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AJQgcjzuwzfL"},"outputs":[],"source":["# Extract the target 'sentiment'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JXtC_bJFw-nr"},"outputs":[],"source":["# Transforming the feature 'review' data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mITBHBOHZw12"},"outputs":[],"source":["# X_cv.shape"]},{"cell_type":"markdown","metadata":{"id":"RFM3ueoMk4Qw"},"source":["#### Method 2: TF-IDF"]},{"cell_type":"markdown","metadata":{"id":"JGQVk1RwDwdG"},"source":["Term Frequency (TF):\n","\n","The TF component measures how often a term appears in a document. It's a raw count of the number of times the term occurs within the document.\n","TF is calculated for each term within each document.\n","\n","Inverse Document Frequency (IDF):\n","\n","The IDF component evaluates how important a term is across the entire corpus(enitre body of text). It's a measure of how unique or rare a term is.\n","Terms that appear frequently in many documents have a lower IDF, while terms that appear in a smaller subset of documents have a higher IDF.\n","\n","TLDR: Considers not only the frequency of words but also their importance across the entire set of documents. It helps in emphasizing words that are more discriminative and less common across documents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"by0RVet1xy29"},"outputs":[],"source":["# Create TFIDF vectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RlpSc6cDx2tO"},"outputs":[],"source":["# Apply TFIDF transformer to 'review' column\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yUjgr06JyDlh"},"outputs":[],"source":["# tfidf.get_feature_names_out()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_e7PzJczyNES"},"outputs":[],"source":["# print(X_tf.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMYTtn3yypB-"},"outputs":[],"source":["# Extract the target 'sentiment'\n"]},{"cell_type":"markdown","metadata":{"id":"kkYW8Q6Ak_tf"},"source":["### Labelling the 'sentiment' text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gF-yWm6nlCOf"},"outputs":[],"source":["# Setting up the LabelBinarizer\n","\n","\n","# # Transforming and Labelling the 'sentiment' data\n","# sentiment_data = lb.fit_transform(data_IMDB['sentiment'])\n","# print(sentiment_data.shape)"]},{"cell_type":"markdown","metadata":{"id":"GGGWtvOMliTf"},"source":["## 4. ML Modelling"]},{"cell_type":"markdown","metadata":{"id":"TOhdJJDZ9cJc"},"source":["### Model 1: Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"3MJbEOPoVm5U"},"source":["#### Logistic Regression - Bags of Words Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sD5S3amNzHRa"},"outputs":[],"source":["# Setting up the LogisticRegression model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Au45psF3D2Ja"},"outputs":[],"source":["# # Split arrays/matrices into random train and test subsets. In this case, 80:20 for Train:Test ratio\n","# x_train_cv, x_test_cv, y_train_cv, y_test_cv = train_test_split(X_cv, Y_cv, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0moO6dI1VlCk"},"outputs":[],"source":["# Fitting the lr model for Bag of Words\n","\n","\n","\n","# Predicting the lr model for Bag of Words\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GZf1teCgVxJM"},"source":["#### Logistic Regression - TFIDF Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVfCailSD7fX"},"outputs":[],"source":["# # Split arrays/matrices into random train and test subsets. In this case, 80:20 for Train:Test ratio\n","# x_train_tf, x_test_tf, y_train_tf, y_test_tf = train_test_split(X_tf, Y_tf, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8Et6na74Dq6"},"outputs":[],"source":["# # Fitting the lr model for TFIDF features\n","# lr_tfidf = lr.fit(x_train_tf, y_train_tf)\n","# print(lr_tfidf)\n","\n","# # Predicting the lr model for TFIDF features\n","# lr_tfidf_predict = lr.predict(x_test_tf)\n","# print(lr_tfidf_predict)"]},{"cell_type":"markdown","metadata":{"id":"I92eyxS364-O"},"source":["#### Logistic Regression - Accuracy Scores & Classification Report for both Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWI9pVdM6zg6"},"outputs":[],"source":["# Accuracy score for Bag of Words\n","\n","\n","\n","# Accuracy score for TFIDF features\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LqVeOEMU7o8B"},"outputs":[],"source":["# Classification report for Bag of Words\n","\n","\n","\n","# Classification report for TFIDF features\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vKvSqPOi8RN-"},"source":["#### Logistic Regression - Confusion Matrix for both Models"]},{"cell_type":"markdown","metadata":{"id":"KcBlCbcgWubg"},"source":["##### For Bags of Words Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHnsIfNE8N6O"},"outputs":[],"source":["\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vqTratyrWxVG"},"source":["##### For TFIDF Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYKRQGjl8atp"},"outputs":[],"source":["# cm_tf = confusion_matrix(y_test_tf, lr_tfidf_predict, labels=lr.classes_)\n","# disp = ConfusionMatrixDisplay(confusion_matrix=cm_tf, display_labels=lr.classes_)\n","# disp.plot()"]},{"cell_type":"markdown","metadata":{"id":"tAELwplY9lRO"},"source":["### Model 2: Multinomial Naive Bayes (MNB)"]},{"cell_type":"markdown","metadata":{"id":"2qBO_5xuWFOn"},"source":["#### MNB - Bags of Words Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5wkjSj99sDu"},"outputs":[],"source":["# Training the Multinomial Naive Bayes model\n","\n","\n","\n","# Fitting the MNB for Bag of Words\n","\n","\n","\n","# Predicting the model for Bag of Words\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HsZAdvhoWKr3"},"source":["#### MNB - TFIDF Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ovMwFrp991vq"},"outputs":[],"source":["# # Fitting the MNB for TFIDF features\n","# mnb_tfidf = mnb.fit(x_train_tf, y_train_tf)\n","# print(mnb_tfidf)\n","\n","# # Predicting the MNB model for TFIDF features\n","# mnb_tfidf_predict = mnb.predict(x_test_tf)\n","# print(mnb_tfidf_predict)"]},{"cell_type":"markdown","metadata":{"id":"-hj3qyuKWOoK"},"source":["#### MNB - Accuracy Scores for both Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxVvLhgd9_Z5"},"outputs":[],"source":["# # Accuracy score for Bag of Words\n","# mnb_bow_score = accuracy_score(y_test_cv, mnb_bow_predict)\n","# print('mnb_bow_score : {:.2f}%'.format(mnb_bow_score*100))\n","\n","# # Accuracy score for TFIDF features\n","# mnb_tfidf_score = accuracy_score(y_test_tf, mnb_tfidf_predict)\n","# print('mnb_tfidf_score : {:.2f}%'.format(mnb_tfidf_score*100))"]},{"cell_type":"markdown","metadata":{"id":"XDWkyGJuWSOE"},"source":["#### MNB - Confusion Matrix for both Models"]},{"cell_type":"markdown","metadata":{"id":"-KQkYOrOWYjT"},"source":["##### For Bags of Words Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fRqQVgDC-mX5"},"outputs":[],"source":["# cm_cv_mnb = confusion_matrix(y_test_cv, mnb_bow_predict, labels=mnb.classes_)\n","# disp = ConfusionMatrixDisplay(confusion_matrix = cm_cv_mnb, display_labels = mnb.classes_)\n","# disp.plot()"]},{"cell_type":"markdown","metadata":{"id":"PastJOQ7WfV1"},"source":["##### For TFIDF Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9M0Yo9Vm_AXk"},"outputs":[],"source":["# cm_tf_mnb = confusion_matrix(y_test_tf, mnb_tfidf_predict, labels=mnb.classes_)\n","# disp = ConfusionMatrixDisplay(confusion_matrix = cm_tf_mnb, display_labels = mnb.classes_)\n","# disp.plot()"]}],"metadata":{"colab":{"collapsed_sections":["XLy9vhfxg4kf","kkYW8Q6Ak_tf","I92eyxS364-O","KcBlCbcgWubg","vqTratyrWxVG"],"provenance":[{"file_id":"1zcrp3gK6repODBPmrSq7T1X1HmQKKlvm","timestamp":1700239150862},{"file_id":"1KXITORjL7oLNg0MqRKuMoLm3iN7UfrcL","timestamp":1699955804539}]},"gpuClass":"standard","kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.0"}},"nbformat":4,"nbformat_minor":0}